{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Eclair for Table and Text Extraction\n",
    "\n",
    "This notebook demonstrates how to build a streamlined document analysis pipeline by strategically combining a local inference endpoint for layout parsing with powerful NVIDIA NIMs (NVIDIA Inference Microservices) for visual analysis. We'll show how to create an end-to-end workflow that transforms unstructured documents into a queryable knowledge base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Models\n",
    "\n",
    "1. `Eclair`: this is a locally hosted model that we will setup in in the next section \n",
    "\n",
    "### Pipeline\n",
    "\n",
    "1. Given an image of the page, the `Eclair` model runs the segmentation and identifies elements on teh page. \n",
    "2. `Eclair` classifies the element roughly as `text`, `table`, or `picture`. \n",
    "3. If an item is identified as a `text`, we simply retrieve the contents extracted by Eclair\n",
    "4. If an item is identified as a `table`, we retrieve the contents exttracted by Eclair. Eclair presents the table as Latex format, so we add an post-processing step to conver that to HTML format for better readability. \n",
    "5. If an item is identified as `picture`, we do not do anything about it. \n",
    "6. The pipeline saves ouput files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "First, let's set up the environment. You'll need to install the required Python libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install requests pillow ipython openai beautifulsoup4 seaborn python-dotenv loguru pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Requirements\n",
    "\n",
    "Eclair can run on 1xL40S, 1xA100 and 1xH100. Smaller GPUs like A10G is also possible but has not been tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Eclair docker container\n",
    "\n",
    "1. Log into docker:\n",
    "\t```bash\n",
    "\tdocker login nvcr.io\n",
    "\t```\n",
    "\t- User name: `$oauthtoken`\n",
    "\t- Password: your NGC API key. \n",
    "2. Pull containers\n",
    "\t```bash\n",
    "\tdocker pull nvcr.io/nim/nvidia/nemotron-parse:latest\n",
    "\t```\n",
    "3. Create cache folder \n",
    "    ```bash\n",
    "    export NGC_API_KEY=<PASTE_API_KEY_HERE>\n",
    "    export LOCAL_NIM_CACHE=~/.cache/nim\n",
    "    mkdir -p $LOCAL_NIM_CACHE\n",
    "    chmod 777 -R $LOCAL_NIM_CACHE\n",
    "    ```\n",
    "4. Launch container\n",
    "    ```bash\n",
    "    docker run -it --rm \\\n",
    "    --gpus \"device=0\" \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -u 0:0 \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -p 8002:8000 \\\n",
    "    nvcr.io/nim/nvidia/nemotron-parse:latest\n",
    "    ```    \n",
    "    Note: \n",
    "    1. Since we only need 1 GPU for the Eclair model, we set `device=0` and `cuda:0`\n",
    "    2. Uvicorn will run on port `8000` inside the container. We map it to host port `8002`\n",
    "5. Wait until it says\n",
    "    ```bash\n",
    "    INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "    ```\n",
    "6. Test health with: \n",
    "    ```bash\n",
    "    curl http://localhost:8002/v1/health/ready\n",
    "    ```\n",
    "    Outputs: \n",
    "    ```json\n",
    "    {\"object\":\"health.response\",\"message\":\"Service is ready.\"}\n",
    "    ```\n",
    "7. Optional, to view the API docs, you can run the command below, and insepct the results in the JSON file: \n",
    "    ```bash\n",
    "    curl -s http://localhost:8002/openapi.json > eclair_api_docs.json\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timezone\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "# load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configs for the Eclair model ---\n",
    "DOCKER_HOST = \"localhost\"  # Updated Docker host IP 10.176.12.87\n",
    "ECLAIR_PORT = 8000            # Your Docker host port\n",
    "ECLAIR_ENDPOINT_PATH = \"/v1/chat/completions\"      # The API endpoint path\n",
    "ECLAIR_ENDPOINT_URL = f\"http://{DOCKER_HOST}:{ECLAIR_PORT}{ECLAIR_ENDPOINT_PATH}\"\n",
    "\n",
    "# --- Output Directory Configuration ---\n",
    "OUTPUT_DIR = \"output_results\"\n",
    "output_annotated_dir = os.path.join(OUTPUT_DIR, \"annotated_images\")\n",
    "output_json_dir = os.path.join(OUTPUT_DIR, \"json_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_like_eclair(image, target_w=1648, target_h=2048):\n",
    "    \"\"\"\n",
    "    Place image on the smallest possible \"paper\" with aspect ratio target_w:target_h.\n",
    "    Don't downscales the image - keeps original size to prevent blurriness.\n",
    "    Place the image in the center of a new canvas. \n",
    "    This canvas should have a w:h ratio = target_w:target_h. ([3, 1648, 2048] is the input image size of Eclair)\n",
    "    The canvas should be the smallest possible size that fits the image without downscaling the image. \n",
    "    Returns:\n",
    "        PIL Image on paper with aspect ratio target_w:target_h\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    img_array = np.array(image)\n",
    "    original_height, original_width = img_array.shape[:2]\n",
    "    \n",
    "    # Calculate target aspect ratio (paper ratio)\n",
    "    target_ratio = target_w / target_h\n",
    "    \n",
    "    # Step 1: Determine if we need to scale up (don't scale down)\n",
    "    scale_for_height = target_h / original_height\n",
    "    scale_for_width = target_w / original_width\n",
    "    scale = max(scale_for_height, scale_for_width, 1.0)  # Never less than 1.0\n",
    "    \n",
    "    # Calculate image dimensions after potential scaling up\n",
    "    if scale > 1.0:\n",
    "        image_width = int(original_width * scale)\n",
    "        image_height = int(original_height * scale)\n",
    "        resized_image = image.resize((image_width, image_height), Image.Resampling.LANCZOS)\n",
    "    else:\n",
    "        # Keep original size (no downscaling)\n",
    "        image_width = original_width\n",
    "        image_height = original_height\n",
    "        resized_image = image\n",
    "    \n",
    "    # Step 2: Calculate smallest paper size with target ratio that fits the image\n",
    "    # paper_width / paper_height = target_ratio\n",
    "    # paper_width >= image_width AND paper_height >= image_height\n",
    "    \n",
    "    # Option 1: Match height, calculate width\n",
    "    paper_h_option1 = image_height\n",
    "    paper_w_option1 = int(paper_h_option1 * target_ratio)\n",
    "    \n",
    "    # Option 2: Match width, calculate height  \n",
    "    paper_w_option2 = image_width\n",
    "    paper_h_option2 = int(paper_w_option2 / target_ratio)\n",
    "    \n",
    "    # Choose the option that fits both dimensions\n",
    "    if paper_w_option1 >= image_width and paper_h_option1 >= image_height:\n",
    "        paper_width = paper_w_option1\n",
    "        paper_height = paper_h_option1\n",
    "    else:\n",
    "        paper_width = paper_w_option2\n",
    "        paper_height = paper_h_option2\n",
    "    \n",
    "    # Step 3: Create paper with white background\n",
    "    resized_array = np.array(resized_image)\n",
    "    \n",
    "    # Handle grayscale images\n",
    "    if len(resized_array.shape) == 2:\n",
    "        resized_array = np.stack([resized_array] * 3, axis=-1)\n",
    "    \n",
    "    # Create white paper\n",
    "    paper_array = np.ones((paper_height, paper_width, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Center the image on the paper\n",
    "    pad_left = (paper_width - image_width) // 2\n",
    "    pad_top = (paper_height - image_height) // 2\n",
    "    \n",
    "    paper_array[pad_top:pad_top+image_height, pad_left:pad_left+image_width] = resized_array\n",
    "    \n",
    "    # Create final image and set DPI to 300\n",
    "    result_image = Image.fromarray(paper_array)\n",
    "    result_image.info['dpi'] = (300, 300)\n",
    "    \n",
    "    return result_image\n",
    "\n",
    "\n",
    "\n",
    "def load_and_resize_image(image_path, save=False, save_dir=None):\n",
    "    \"\"\"\n",
    "    Load an image from the given path and resize it to 2048x1648 (height x width).\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the input image\n",
    "        save (bool): If True, save the resized image with a '_resized' suffix\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the resized image (either saved or original path)\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import os\n",
    "    \n",
    "    # Load the image, convert to RGB if necessary\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Resize to (width, height) - PIL uses (width, height) format\n",
    "    # Target: height=2048, width=1648\n",
    "    # target_size = (1648, 2048)  # (width, height) for PIL\n",
    "    # resized_img = img.resize(target_size ,Image.Resampling.LANCZOS)\n",
    "    resized_img = preprocess_image_like_eclair(img)\n",
    "\n",
    "    if save:\n",
    "        # Create resized file path\n",
    "        path_parts = os.path.splitext(image_path)\n",
    "        # if save_dir is not None, use it\n",
    "        if save_dir is not None:\n",
    "            # get base file name\n",
    "            base_name = os.path.basename(image_path)\n",
    "            # split the file name into name and extension\n",
    "            name, ext = os.path.splitext(base_name)\n",
    "            resized_path = os.path.join(save_dir, f\"{name}_resized{ext}\")\n",
    "        else:\n",
    "            resized_path = f\"{path_parts[0]}_resized{path_parts[1]}\"\n",
    "        \n",
    "        # Save the resized image\n",
    "        resized_img.save(resized_path)\n",
    "        return resized_path\n",
    "    else:\n",
    "        return image_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test resize funct4ion\n",
    "# result = load_and_resize_image(\"/home/ubuntu/nim-dev/models/nemo-retriever-parse/output_results/converted_images/soa_1/fitz/soa_1_page_1_fitz.png\"  , save=True, save_dir=\"output_results/resized_images\")\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collect Example Document Images\n",
    "\n",
    "In this step, we collect the example images we want to analyze in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, itertools\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Root of the notebook\n",
    "WORKSPACE_ROOT = Path.cwd()\n",
    "# Root of the converted imaages folder, from step 1 notebook\n",
    "DATA_ROOT = WORKSPACE_ROOT / \"output_results\" \n",
    "\n",
    "# Patterns covering both pipelines and direct converted images\n",
    "# adjust accordingly to match the location of your converted images\n",
    "PATTERNS = [\n",
    "    # str(DATA_ROOT / \"converted_images\" / \"soa_1\" / \"fitz\" / \"*.png\"),\n",
    "    str(DATA_ROOT / \"converted_images\" / \"a_map\" / \"fitz\" / \"*.png\")\n",
    "]\n",
    "\n",
    "def get_all_paths(patterns, data_root, workspace_root, resize=True):\n",
    "\n",
    "    \"\"\"\n",
    "    This function gets all the paths of the converted images, and resize them if needed\n",
    "    patterns: list of patterns to match the converted images\n",
    "    data_root: str, the root of the converted images\n",
    "    workspace_root: str, the root of the workspace\n",
    "    resize: bool, whether to resize the images\n",
    "    returns: list of paths to the processed images\n",
    "    \"\"\"\n",
    "\n",
    "    # get all the paths of the converted images\n",
    "    all_paths_abs = []\n",
    "    for pat in patterns:\n",
    "        all_paths_abs.extend(glob(pat, recursive=True))\n",
    "\n",
    "    # Dedup + natural sort (so _page_2_ comes before _page_10_)\n",
    "    def _natural_key(s: str):\n",
    "        return [int(t) if t.isdigit() else t.lower() for t in re.split(r\"(\\d+)\", s)]\n",
    "    all_paths_abs = sorted(set(all_paths_abs), key=_natural_key)\n",
    "\n",
    "    # create a folder to save the resized images\n",
    "    save_dir = data_root / \"resized_images\"\n",
    "    if save_dir is not None:\n",
    "        # delete the file if it exists\n",
    "        if os.path.exists(save_dir):\n",
    "            # delete directory and all its contents\n",
    "            shutil.rmtree(save_dir)\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # add resized images to the list\n",
    "    allowed_page_paths = []\n",
    "    for p in all_paths_abs:\n",
    "        if resize: \n",
    "            resized_fp = load_and_resize_image(p, save=True, save_dir=save_dir)\n",
    "            # convert to repo‑relative paths\n",
    "            rel = os.path.relpath(resized_fp, workspace_root).replace(\"\\\\\", \"/\")\n",
    "        else: \n",
    "            rel = os.path.relpath(p, workspace_root).replace(\"\\\\\", \"/\")\n",
    "        allowed_page_paths.append(rel)\n",
    "\n",
    "    # sort the paths\n",
    "    allowed_page_paths = sorted(set(allowed_page_paths), key=_natural_key)\n",
    "\n",
    "    print(f\"✅ Collected a total of {len(allowed_page_paths)} image pages\")\n",
    "    print(\"Here are the first 10 input image paths:\")\n",
    "    for sample in itertools.islice(allowed_page_paths, 10):\n",
    "        print(\" -\", sample)\n",
    "        \n",
    "    return allowed_page_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collected a total of 43 image pages\n",
      "Here are the first 10 input image paths:\n",
      " - output_results/resized_images/a_map_page_1_fitz_resized.png\n",
      " - output_results/resized_images/a_map_page_2_fitz_resized.png\n",
      " - output_results/resized_images/a_map_page_3_fitz_resized.png\n",
      " - output_results/resized_images/a_map_page_4_fitz_resized.png\n",
      " - output_results/resized_images/a_map_page_5_fitz_resized.png\n",
      " - output_results/resized_images/a_map_page_6_fitz_resized.png\n",
      " - output_results/resized_images/a_map_page_7_fitz_resized.png\n",
      " - output_results/resized_images/a_map_page_8_fitz_resized.png\n",
      " - output_results/resized_images/a_map_page_9_fitz_resized.png\n",
      " - output_results/resized_images/a_map_page_10_fitz_resized.png\n"
     ]
    }
   ],
   "source": [
    "# get hte input file paths\n",
    "ALLOWED_PAGE_PATHS = get_all_paths(\n",
    "    patterns=PATTERNS, \n",
    "    data_root=DATA_ROOT, \n",
    "    workspace_root=WORKSPACE_ROOT, \n",
    "    resize=True # set to False if you want to use the original images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Core Pipeline in Action\n",
    "\n",
    "Now, let's walk through the code that powers our pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "This block encapsulates our pipeline's logic. \n",
    "\n",
    "1. `call_eclair_inference` handles communication with our local layout analysis engine. \n",
    "2. `latex_table_to_html` provides advanced conversion of LaTeX tables to HTML. \n",
    "3. Finally, `draw_annotations` visualizes the results by drawing labeled bounding boxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eclair related helpers\n",
    "\n",
    "The Eclair model will return a string, with bbox and extracted context. We need to parse it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "_PATTERN = re.compile(\n",
    "    r\"<x_(\\d+(?:\\.\\d+)?)><y_(\\d+(?:\\.\\d+)?)>(.*?)<x_(\\d+(?:\\.\\d+)?)><y_(\\d+(?:\\.\\d+)?)><class_([^>]+)>\",\n",
    "    re.DOTALL,\n",
    ")\n",
    "\n",
    "def parse_content_to_blocks(content: str) -> List[Dict[str, Any]]:\n",
    "    transformed_data: List[Dict[str, Any]] = []\n",
    "    for xmin, ymin, text, xmax, ymax, cls in _PATTERN.findall(content):\n",
    "        transformed_data.append({\n",
    "            \"type\": cls,  # same role as old 'category'\n",
    "            \"text\": text.strip(),\n",
    "            \"bbox\": {\n",
    "                \"xmin\": float(xmin),\n",
    "                \"ymin\": float(ymin),\n",
    "                \"xmax\": float(xmax),\n",
    "                \"ymax\": float(ymax),\n",
    "            },\n",
    "        })\n",
    "    return transformed_data\n",
    "\n",
    "def parse_response(resp: Union[List, Dict]) -> List[Dict[str, Any]]:\n",
    "    \n",
    "    # in rc1.5, use this: \n",
    "    content = resp['choices'][0][\"message\"][\"content\"]\n",
    "    return parse_content_to_blocks(content)\n",
    "\n",
    "    # in GA 1.0, use this: \n",
    "    # return json.loads(resp['choices'][0]['message']['tool_calls'][0]['function']['arguments'])[0]\n",
    "\n",
    "def encode_file_to_base64(image_path: str):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    ext = os.path.splitext(image_path)[1].lower()\n",
    "    if ext in ('.jpg', '.jpeg'):\n",
    "        mime = 'image/jpeg'\n",
    "    elif ext == '.png':\n",
    "        mime = 'image/png'\n",
    "    else:\n",
    "        # raise warning\n",
    "        mime = 'image/' + ext\n",
    "        print(f\"Warning: Imeage extension is {ext}. Not all image types are supported. It might be best to convert to .png, .jpg, .jpeg instead. For now we will try to encode it as {mime}\")\n",
    "\n",
    "    return \"data:\" + mime + \";base64,\" + image_b64\n",
    "\n",
    "def call_eclair_inference(image_path: str, temperature: float = 0.5):\n",
    "    \"\"\"\n",
    "    Sends an image to the local Docker inference endpoint and transforms the\n",
    "    output to the format expected by the rest of the script.\n",
    "    Args:\n",
    "        image_path (str): The path to the image to send to the Docker endpoint.\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a block in the image.\n",
    "    \"\"\"\n",
    "    image_filename = os.path.basename(image_path)\n",
    "\n",
    "    # convert to base64 encoded image\n",
    "    image_b64 = encode_file_to_base64(image_path)\n",
    "    print(f\"    - Sending {image_filename} to Docker endpoint: {ECLAIR_ENDPOINT_URL}\")\n",
    "\n",
    "    try:\n",
    "        payload = {\n",
    "            \"model\": \"nvidia/NVIDIA-Nemotron-Parse-v1.1\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"</s><s><predict_bbox><predict_classes><output_markdown>\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": image_b64}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0,\n",
    "            \"top_k\": 1,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"max_tokens\": 8000,\n",
    "            \"skip_special_tokens\": False\n",
    "        }\n",
    "        response = requests.post(ECLAIR_ENDPOINT_URL, json=payload)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        response_json = response.json()\n",
    "\n",
    "        # DEBUG\n",
    "        # return response_json\n",
    "        transformed_data = parse_response(response_json)\n",
    "        return transformed_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"    - ERROR: Could not connect to Docker endpoint at {ECLAIR_ENDPOINT_URL}. Please ensure it is running. Details: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"    - An unexpected error occurred during Docker inference: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test eclair with a single image\n",
    "\n",
    "# for i in range(1, 43):\n",
    "#     parsed_data = call_eclair_inference(\n",
    "#         image_path = f\"/home/ubuntu/nim-dev/models/nemotron-parse-prod-hf/output_results/resized_images/soa_1_page_1_fitz_resized.png\", \n",
    "#         temperature=0\n",
    "#     )\n",
    "#     print(parsed_data)\n",
    "#     break\n",
    "    # finish_reason = parsed_data['choices'][0]['finish_reason']\n",
    "    # print(f\"Page {i} finished because of {finish_reason}\")\n",
    "    # if finish_reason != 'stop': \n",
    "    #     print(f\"Page {i} finished because of {finish_reason}\")\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawing related helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:124: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:169: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:124: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:169: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/tmp/ipykernel_702866/356512300.py:124: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  Parse a \\multirow or \\multicolumn command starting at index i.\n",
      "/tmp/ipykernel_702866/356512300.py:169: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  Process an arbitrary LaTeX text string and look for occurrences of \\multirow or \\multicolumn commands.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "# Define the colors for each class. This is used to color the bounding boxes in the output image.\n",
    "CLASS_COLORS = {\n",
    "    'Text': '#4CAF50', 'Title': '#D32F2F', 'Section-header': '#E91E63',\n",
    "    'List-item': '#1976D2', 'Table': '#03A9F4', 'Picture': '#6D4C41',\n",
    "    'Caption': '#607D8B', 'Formula': '#FF9800', 'Page-header': '#BDBDBD',\n",
    "    'Page-footer': '#BDBDBD', 'Footnote': '#00BCD4', 'Bibliography': '#512DA8',\n",
    "    'TOC': '#FFC107', 'DEFAULT': '#9E9E9E'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def get_text_color(hex_color):\n",
    "    \"\"\"\n",
    "    Gets the text color for a given hex color. Used to determine the color of the text in the bounding box.\n",
    "    Args:\n",
    "        hex_color (str): The hex color to get the text color for.\n",
    "    Returns:\n",
    "        str: The text color.\n",
    "    \"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    r, g, b = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "    luminance = (0.2126 * r + 0.7152 * g + 0.0722 * b)\n",
    "    return \"#000000\" if luminance > 140 else \"#FFFFFF\"\n",
    "\n",
    "def draw_annotations(image: Image.Image, extraction_data: list):\n",
    "    \"\"\"\n",
    "    Draws the annotations on the image.\n",
    "    Args:\n",
    "        image (Image.Image): The image to draw the annotations on.\n",
    "        extraction_data (list): The data to draw the annotations on.\n",
    "    Returns:\n",
    "        Image.Image: The image with the annotations drawn on it.\n",
    "    \"\"\"\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = image.size\n",
    "    \n",
    "    box_thickness = max(3, int(width / 600))\n",
    "    font_size = max(20, int(width / 100))\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"Arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for i, item in enumerate(extraction_data):\n",
    "        bbox = item.get(\"bbox\")\n",
    "        if not bbox or bbox.get(\"xmax\", 0) < bbox.get(\"xmin\", 0) or bbox.get(\"ymax\", 0) < bbox.get(\"ymin\", 0):\n",
    "            continue\n",
    "\n",
    "        item_type = item.get(\"type\", \"DEFAULT\")\n",
    "        color = CLASS_COLORS.get(item_type, CLASS_COLORS['DEFAULT'])\n",
    "        text_color = get_text_color(color)\n",
    "\n",
    "        left, top, right, bottom = (\n",
    "            bbox[\"xmin\"] * width, \n",
    "            bbox[\"ymin\"] * height, \n",
    "            bbox[\"xmax\"] * width, \n",
    "            bbox[\"ymax\"] * height\n",
    "        )\n",
    "        # this `transform_bbox_to_original` function may also be used to remap the bbox to the original image size , if you do not resize the image before inputting it into Eclair. \n",
    "        # however it is not as reliable as trasnforming the input image to the appropriate size for Eclair. \n",
    "        # if you already resized the image, then you no longer need to use this function\n",
    "        # left, top, right, bottom = transform_bbox_to_original(bbox, width, height)\n",
    "        draw.rectangle([left, top, right, bottom], outline=color, width=box_thickness)\n",
    "\n",
    "        label = f\"ID: {i} | {item_type}\"\n",
    "        if item_type == 'Picture':\n",
    "            label = f\"ID: {i} | Picture:{item.get('sub_type', 'general')}\"\n",
    "\n",
    "        text_bbox = draw.textbbox((0, 0), label, font=font)\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "\n",
    "        label_y_pos = top - text_height - (box_thickness * 2)\n",
    "        if label_y_pos < 0: label_y_pos = top + (box_thickness * 2)\n",
    "\n",
    "        label_bg = (left, label_y_pos, left + text_width + 10, label_y_pos + text_height + 10)\n",
    "        draw.rectangle(label_bg, fill=color)\n",
    "        draw.text((left + 5, label_y_pos + 5), label, fill=text_color, font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "# --- Advanced LaTeX Table to HTML Conversion Functions ---\n",
    "def skip_whitespace(text, i):\n",
    "    \"\"\"Advance index i past any whitespace.\"\"\"\n",
    "    while i < len(text) and text[i].isspace():\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "def parse_braced_argument(text, i):\n",
    "    \"\"\"\n",
    "    Given text and an index i that should point at an opening '{',\n",
    "    return a tuple (argument_content, new_index) where argument_content is the full\n",
    "    string inside the balanced braces and new_index is the position just after the matching '}'.\n",
    "    Args:\n",
    "        text (str): The text to parse.\n",
    "        i (int): The index to start parsing at.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the argument content and the new index.\n",
    "    \"\"\"\n",
    "    if i >= len(text) or text[i] != '{':\n",
    "        raise ValueError(\"Expected '{' at position {}\".format(i))\n",
    "    i += 1  # skip the opening brace\n",
    "    start = i\n",
    "    level = 1\n",
    "    while i < len(text) and level > 0:\n",
    "        if text[i] == '{':\n",
    "            level += 1\n",
    "        elif text[i] == '}':\n",
    "            level -= 1\n",
    "        i += 1\n",
    "    if level != 0:\n",
    "        raise ValueError(\"Unbalanced braces starting at position {}\".format(start-1))\n",
    "    # The argument content is from start to i-1 (excluding the closing brace)\n",
    "    return text[start:i-1], i\n",
    "\n",
    "def parse_command(text, i):\n",
    "    \"\"\"\n",
    "    Parse a \\multirow or \\multicolumn command starting at index i.\n",
    "    This function assumes the command has exactly three braced arguments.\n",
    "\n",
    "    It processes each argument recursively. For the third argument, after recursive processing,\n",
    "    it replaces any unescaped & with \\&.\n",
    "    \n",
    "    Args: \n",
    "        text (str): The text to parse.\n",
    "        i (int): The index to start parsing at.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the command text and the new index.\n",
    "    \"\"\"\n",
    "    # Determine which command we have.\n",
    "    if text.startswith(r\"\\multirow\", i):\n",
    "        command_name = r\"\\multirow\"\n",
    "        i += len(r\"\\multirow\")\n",
    "    elif text.startswith(r\"\\multicolumn\", i):\n",
    "        command_name = r\"\\multicolumn\"\n",
    "        i += len(r\"\\multicolumn\")\n",
    "    else:\n",
    "        raise ValueError(\"Expected \\\\multirow or \\\\multicolumn at position {}\".format(i))\n",
    "\n",
    "    # Skip whitespace between the command name and the first argument.\n",
    "    i = skip_whitespace(text, i)\n",
    "    args = []\n",
    "    # Expect exactly three arguments\n",
    "    for arg_index in range(3):\n",
    "        if i >= len(text) or text[i] != '{':\n",
    "            raise ValueError(\"Expected '{{' for argument {} at position {}\".format(arg_index+1, i))\n",
    "        arg_content, i = parse_braced_argument(text, i)\n",
    "        # Process the content recursively to catch nested commands\n",
    "        processed_arg = clean_multi_cells(arg_content)\n",
    "        if arg_index == 2:\n",
    "            # For the cell text (third argument), replace any unescaped &\n",
    "            processed_arg = re.sub(r'(?<!\\\\)&', r'\\\\&', processed_arg)\n",
    "        args.append(processed_arg)\n",
    "        # Only skip whitespace between arguments, not after the last one.\n",
    "        if arg_index < 2:\n",
    "            i = skip_whitespace(text, i)\n",
    "    # Reconstruct the full command with its three arguments\n",
    "    command_text = f\"{command_name}{{{args[0]}}}{{{args[1]}}}{{{args[2]}}}\"\n",
    "    return command_text, i\n",
    "\n",
    "def clean_multi_cells(text):\n",
    "    \"\"\"\n",
    "    Process an arbitrary LaTeX text string and look for occurrences of \\multirow or \\multicolumn commands.\n",
    "    When found, the command is parsed (handling nested braces and nested commands) and its third argument is fixed.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process.\n",
    "    Returns:\n",
    "        str: The processed text.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        # Find next occurrence of either command.\n",
    "        idx_multi = text.find(r\"\\multirow\", i)\n",
    "        idx_multiC = text.find(r\"\\multicolumn\", i)\n",
    "\n",
    "        # Determine the next index among the two (if any)\n",
    "        if idx_multi == -1 and idx_multiC == -1:\n",
    "            result.append(text[i:])\n",
    "            break\n",
    "        if idx_multi == -1:\n",
    "            next_idx = idx_multiC\n",
    "        elif idx_multiC == -1:\n",
    "            next_idx = idx_multi\n",
    "        else:\n",
    "            next_idx = min(idx_multi, idx_multiC)\n",
    "\n",
    "        # Append text before the command (preserving any whitespace)\n",
    "        result.append(text[i:next_idx])\n",
    "        # Process the command starting at next_idx\n",
    "        command_text, new_index = parse_command(text, next_idx)\n",
    "        result.append(command_text)\n",
    "        i = new_index\n",
    "    return ''.join(result)\n",
    "\n",
    "def parse_brace(s, pos):\n",
    "    \"\"\"\n",
    "    Given a string s and an index pos pointing to an opening '{',\n",
    "    returns a tuple (content, new_pos) where content is the string\n",
    "    between the matching braces (handling nested braces) and new_pos is\n",
    "    the index just after the closing '}'.\n",
    "\n",
    "    Args:\n",
    "        s (str): The string to parse.\n",
    "        pos (int): The index to start parsing at.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the content and the new index.\n",
    "    \"\"\"\n",
    "    if pos >= len(s) or s[pos] != '{':\n",
    "        raise ValueError(\"Expected '{{' at position %d\" % pos)\n",
    "    pos += 1  # skip the opening brace\n",
    "    content = \"\"\n",
    "    depth = 1\n",
    "    while pos < len(s) and depth:\n",
    "        char = s[pos]\n",
    "        if char == '{':\n",
    "            depth += 1\n",
    "            content += char\n",
    "        elif char == '}':\n",
    "            depth -= 1\n",
    "            if depth:\n",
    "                content += char\n",
    "        else:\n",
    "            content += char\n",
    "        pos += 1\n",
    "    if depth != 0:\n",
    "        raise ValueError(\"Unmatched '{{' in string.\")\n",
    "    return content, pos\n",
    "\n",
    "def parse_command_merge(s, pos):\n",
    "    \"\"\"\n",
    "    Parse a multirow or multicolumn command starting at s[pos]. If the content\n",
    "    of the command contains a nested command, then recursively parse the inner\n",
    "    command and merge its parameters with the outer ones. The merging is done\n",
    "    so that the outer multirow's parameters (e.g. rowspan and width) are kept\n",
    "    while the inner command's parameters (e.g. colspan, alignment) and its innermost\n",
    "    content are returned.\n",
    "\n",
    "    Args: \n",
    "        s (str): The string to parse.\n",
    "        pos (int): The index to start parsing at.\n",
    "\n",
    "    Returns a tuple (merged_dict, new_pos) where merged_dict is a dictionary\n",
    "    containing the combined parameters and new_pos is the updated index after\n",
    "    parsing the command.\n",
    "    \"\"\"\n",
    "    if s.startswith(r\"\\multirow\", pos):\n",
    "        newpos = pos + len(r\"\\multirow\")\n",
    "        # Parse the three required arguments for multirow: rowspan, width, and content.\n",
    "        rowspan, newpos = parse_brace(s, newpos)\n",
    "        width, newpos = parse_brace(s, newpos)\n",
    "        content, newpos = parse_brace(s, newpos)\n",
    "        # Look for a nested command (either \\multirow or \\multicolumn) in the content.\n",
    "        index_mr = content.find(r\"\\multirow\")\n",
    "        index_mc = content.find(r\"\\multicolumn\")\n",
    "        if index_mr == -1 and index_mc == -1:\n",
    "            # No nested command found; return this command's details.\n",
    "            return {\"rowspan\": rowspan.strip(), \"width\": width.strip(), \"content\": content.strip()}, newpos\n",
    "        else:\n",
    "            # At least one nested command is present. Pick the first occurrence.\n",
    "            indices = [i for i in (index_mr, index_mc) if i != -1]\n",
    "            first_index = min(indices)\n",
    "            # Parse the inner (nested) command from within the content.\n",
    "            inner, _ = parse_command_merge(content, first_index)\n",
    "            # Merge: keep the outer multirow's parameters and add the inner ones.\n",
    "            merged = {\"rowspan\": rowspan.strip(), \"width\": width.strip()}\n",
    "            merged.update(inner)\n",
    "            return merged, newpos\n",
    "\n",
    "    elif s.startswith(r\"\\multicolumn\", pos):\n",
    "        newpos = pos + len(r\"\\multicolumn\")\n",
    "        # Parse the three arguments for multicolumn: colspan, alignment, and content.\n",
    "        colspan, newpos = parse_brace(s, newpos)\n",
    "        alignment, newpos = parse_brace(s, newpos)\n",
    "        content, newpos = parse_brace(s, newpos)\n",
    "        # Look for a nested command in the content.\n",
    "        index_mr = content.find(r\"\\multirow\")\n",
    "        index_mc = content.find(r\"\\multicolumn\")\n",
    "        if index_mr == -1 and index_mc == -1:\n",
    "            return {\"colspan\": colspan.strip(), \"alignment\": alignment.strip(), \"content\": content.strip()}, newpos\n",
    "        else:\n",
    "            indices = [i for i in (index_mr, index_mc) if i != -1]\n",
    "            first_index = min(indices)\n",
    "            inner, _ = parse_command_merge(content, first_index)\n",
    "            merged = {\"colspan\": colspan.strip(), \"alignment\": alignment.strip()}\n",
    "            merged.update(inner)\n",
    "            return merged, newpos\n",
    "\n",
    "    # Not a recognized command starting at pos.\n",
    "    return None, pos\n",
    "\n",
    "def extract_merged_commands(s):\n",
    "    \"\"\"\n",
    "    Scan through the LaTeX string s and extract merged multirow/multicolumn commands.\n",
    "    For each command found, if there is nesting the parser merges the outer and inner\n",
    "    parameters so that the final result includes both the rowspan (or width) and the colspan\n",
    "    (or alignment) along with the innermost content.\n",
    "\n",
    "    Args: \n",
    "        s (str): The string to parse.\n",
    "    Returns a list of dictionaries.\n",
    "    \"\"\"\n",
    "    pos = 0\n",
    "    results = []\n",
    "    while pos < len(s):\n",
    "        if s[pos] == '\\\\':\n",
    "            res, newpos = parse_command_merge(s, pos)\n",
    "            if res is not None:\n",
    "                results.append(res)\n",
    "                pos = newpos\n",
    "                continue\n",
    "        pos += 1\n",
    "    return results\n",
    "\n",
    "def remove_tags(html, tags_to_remove):\n",
    "    \"\"\"\n",
    "    Removes the specified tags from the HTML string.\n",
    "\n",
    "    Args:\n",
    "        html (str): The HTML string to remove tags from.\n",
    "        tags_to_remove (list): A list of tag names to remove.\n",
    "    Returns:\n",
    "        str: The HTML string with the specified tags removed.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Loop through the tags to remove\n",
    "    for tag_name in tags_to_remove:\n",
    "        for tag in soup.find_all(tag_name):\n",
    "            # Move the children of the tag to the parent tag\n",
    "            tag.unwrap()  # This removes the tag but keeps its contents\n",
    "    # Return the modified HTML as a string\n",
    "    return str(soup)\n",
    "\n",
    "def convert_th_to_td(html):\n",
    "    \"\"\"Replace all th tags with td tags\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html)\n",
    "    for th_tag in soup.find_all('th'):\n",
    "        th_tag.name = 'td'\n",
    "    return str(soup)\n",
    "\n",
    "def replace_italic(text):\n",
    "    pattern = re.compile(r'(?<!\\\\)_(.*?)(?<!\\\\)_')\n",
    "\n",
    "    def italic_replacer(match):\n",
    "        # Get the text inside the underscores.\n",
    "        content = match.group(1)\n",
    "        # Remove the escape (backslash) from any escaped underscores inside.\n",
    "        content = content.replace(r'\\_', '_')\n",
    "        return f\"<i>{content}</i>\"\n",
    "\n",
    "    # Replace all occurrences of the pattern using the replacer function.\n",
    "    return pattern.sub(italic_replacer, text)\n",
    "\n",
    "def replace_bold(text):\n",
    "    pattern = re.compile(r'(?<!\\\\)\\*\\*(.*?)(?<!\\\\)\\*\\*')\n",
    "\n",
    "    def bold_replacer(match):\n",
    "        content = match.group(1)\n",
    "        # Unescape any escaped asterisks within the captured text.\n",
    "        content = content.replace(r'\\*', '*')\n",
    "        return f\"<b>{content}</b>\"\n",
    "\n",
    "    return pattern.sub(bold_replacer, text)\n",
    "\n",
    "def latex_table_to_html(latex_str):\n",
    "    # Pattern to match the entire tabular environment\n",
    "    table_pattern = r'\\\\begin{tabular}{([^}]*)}\\s*(.*?)\\\\end{tabular}'\n",
    "\n",
    "    def process_cell(cell):\n",
    "        # Clean up cell content\n",
    "        cell = cell.strip()\n",
    "\n",
    "        out = extract_merged_commands(cell)\n",
    "        if len(out) > 0:\n",
    "            cell = process_cell(out[0][\"content\"])[\"content\"]\n",
    "            rowspan = int(out[0].get(\"rowspan\", \"1\"))\n",
    "            colspan = int(out[0].get(\"colspan\", \"1\"))\n",
    "            return {\n",
    "                \"content\": cell,\n",
    "                \"colspan\": colspan,\n",
    "                \"rowspan\": rowspan\n",
    "            }\n",
    "\n",
    "        # Replace latex and markdown formatting with HTML tags\n",
    "        cell = re.sub(r'\\$([^$]*)\\$', r'\\1', cell)  # Remove math mode\n",
    "        cell = re.sub(r'\\\\textbf{([^}]*)}', r'<b>\\1</b>', cell)  # Convert latex bold\n",
    "        cell = re.sub(r'\\\\textit{([^}]*)}', r'<i>\\1</i>', cell)  # Convert latex italic\n",
    "        cell = replace_italic(cell)\n",
    "        cell = replace_bold(cell)\n",
    "\n",
    "        cell = cell.replace(\"\\\\$\", \"$\").replace(\"\\\\%\", \"%\").replace(\"\\\\newline\", \" \").replace(\"\\\\textless\", \"<\").replace(\"\\\\textgreater\", \">\").replace(\"\\\\*\", \"*\").replace(\"\\\\_\", \"_\").replace(\"\\\\\\\\\", \"\\\\\")\n",
    "\n",
    "        # Replace \\& with & in the cell text\n",
    "        cell = cell.replace(r'\\\\&', '&')\n",
    "\n",
    "        return {\n",
    "            'content': cell,\n",
    "            'colspan': 1,\n",
    "            'rowspan': 1\n",
    "        }\n",
    "\n",
    "    def split_row(input_string):\n",
    "        # Use a regular expression to split on '&' that is not preceded by a backslash\n",
    "        return re.split(r'(?<!\\\\)&', input_string)\n",
    "\n",
    "    def convert_table(match):\n",
    "        # Extract table content\n",
    "        format_spec, content = match.groups()\n",
    "\n",
    "        # Start building HTML table\n",
    "        html = ['<table>']\n",
    "\n",
    "        # Track cells for multirow\n",
    "        multirow_tracker = set()\n",
    "\n",
    "        # Process rows\n",
    "        rows = re.split(r'\\\\\\\\', content)\n",
    "        current_row = 0\n",
    "\n",
    "        for row in rows:\n",
    "            if not row.strip():\n",
    "                continue\n",
    "\n",
    "            row = row.strip()\n",
    "\n",
    "            # Skip \\hline\n",
    "            if '\\\\hline' in row:\n",
    "                row = row.replace('\\\\hline', '')\n",
    "                if not row.strip():\n",
    "                    continue\n",
    "\n",
    "            row = clean_multi_cells(row)\n",
    "\n",
    "            # Process cells\n",
    "            cells = split_row(row)\n",
    "            processed_cells = [process_cell(cell) for cell in cells]\n",
    "\n",
    "            # Build row HTML\n",
    "            html.append('  <tr>')\n",
    "            current_col = 0\n",
    "\n",
    "            # Handle cells in this row\n",
    "            for cell in processed_cells:\n",
    "\n",
    "                # Add the current cell\n",
    "                attrs = []\n",
    "                if cell['colspan'] > 1:\n",
    "                    attrs.append(f'colspan=\"{cell[\"colspan\"]}\"')\n",
    "                if cell['rowspan'] > 1:\n",
    "                    attrs.append(f'rowspan=\"{cell[\"rowspan\"]}\"')\n",
    "                    # Add to tracker for future rows\n",
    "                    for r in range(current_row + 1, current_row + cell['rowspan']):\n",
    "                        for c in range(current_col, current_col + cell['colspan']):\n",
    "                            multirow_tracker.add((r, c))\n",
    "\n",
    "                if (current_row, current_col) in multirow_tracker and cell[\"content\"] == \"\" and cell[\"colspan\"] == 1 and cell[\"rowspan\"] == 1:\n",
    "                    current_col += cell['colspan']\n",
    "                    continue\n",
    "\n",
    "                attr_str = ' ' + ' '.join(attrs) if attrs else ''\n",
    "                cell_tag = 'td'\n",
    "                html.append(f'    <{cell_tag}{attr_str}>{cell[\"content\"]}</{cell_tag}>')\n",
    "\n",
    "                current_col += cell['colspan']\n",
    "\n",
    "            html.append('  </tr>')\n",
    "            current_row += 1\n",
    "\n",
    "        html.append('</table>')\n",
    "        return '\\n'.join(html)\n",
    "\n",
    "    # Convert all tabular environments in the input\n",
    "    return re.sub(table_pattern, convert_table, latex_str, flags=re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the Pipeline\n",
    "\n",
    "This cell is the pipeline's orchestrator. \n",
    "\n",
    "1. It iterates through each image path, calling the local Docker endpoint for initial layout parsing. \n",
    "2. All results are aggregated into structured JSON objects. \n",
    "3. `draw_annotations` is used to create a visualized output for each page. \n",
    "4. Additionally, it shows the HTML rendered content of detected tables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_results/resized_images/a_map_page_1_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_2_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_3_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_4_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_5_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_6_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_7_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_8_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_9_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_10_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_11_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_12_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_13_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_14_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_15_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_16_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_17_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_18_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_19_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_20_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_21_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_22_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_23_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_24_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_25_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_26_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_27_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_28_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_29_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_30_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_31_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_32_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_33_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_34_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_35_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_36_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_37_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_38_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_39_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_40_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_41_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_42_fitz_resized.png',\n",
       " 'output_results/resized_images/a_map_page_43_fitz_resized.png']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALLOWED_PAGE_PATHS = ALLOWED_PAGE_PATHS[:5]\n",
    "ALLOWED_PAGE_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing Page 1/43: output_results/resized_images/a_map_page_1_fitz_resized.png\n",
      "[Stage 1] Calling local parser for layout analysis...\n",
      "    - Sending a_map_page_1_fitz_resized.png to Docker endpoint: http://localhost:8000/v1/chat/completions\n"
     ]
    }
   ],
   "source": [
    "# make output folders\n",
    "for d in [output_annotated_dir, output_json_dir]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "# initialize empty list to store all document data\n",
    "all_document_data = []\n",
    "total_pages = len(ALLOWED_PAGE_PATHS)\n",
    "\n",
    "# iterate through each image path\n",
    "for i, image_path_str in enumerate(ALLOWED_PAGE_PATHS):\n",
    "    page_count = i + 1\n",
    "    print(f\"🔍 Processing Page {page_count}/{total_pages}: {image_path_str}\")\n",
    "\n",
    "    # if the file does not exist, fail silently\n",
    "    if not os.path.exists(image_path_str):\n",
    "        print(f\" - ❌ ERROR: File not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # read the image\n",
    "    page_image = Image.open(image_path_str)\n",
    "    filename_stem = os.path.splitext(os.path.basename(image_path_str))[0]\n",
    "\n",
    "    # Stage 1: Call the Docker inference function for layout analysis\n",
    "    print(\"[Stage 1] Calling local parser for layout analysis...\")\n",
    "    extracted_data = call_eclair_inference(\n",
    "        image_path_str, \n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    page_num_str = ''.join(filter(str.isdigit, filename_stem.split('_')[-1]))\n",
    "    page_num = int(page_num_str) if page_num_str.isdigit() else 0\n",
    "\n",
    "    # initialize the page entry to store the extracted data\n",
    "    page_entry = {\n",
    "        \"source_document\": image_path_str,\n",
    "        \"source_page_number\": page_num,\n",
    "        \"processing_timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"status\": \"\",\n",
    "        \"content\": []\n",
    "    }\n",
    "\n",
    "    # if the layout extraction failed, append the failed entry and continue\n",
    "    if extracted_data is None:\n",
    "        page_entry[\"status\"] = \"Layout extraction failed (Docker Inference)\"\n",
    "        all_document_data.append(page_entry) # Append failed entry and continue\n",
    "        continue\n",
    "\n",
    "    # if the layout extraction was successful, append the successful entry\n",
    "    else:\n",
    "        page_entry[\"status\"] = \"Layout extraction successful (Docker Inference)\"\n",
    "        print(f\"[Stage 1] Found {len(extracted_data)} document objects.\")\n",
    "\n",
    "    # Draw annotations and save the visualized output\n",
    "    annotated_image = draw_annotations(page_image.copy(), extracted_data)\n",
    "    output_filename = os.path.join(output_annotated_dir, f\"{filename_stem}_annotated.png\")\n",
    "    annotated_image.save(output_filename)\n",
    "    print(\"\\nDisplaying final annotated page:\")\n",
    "    display(annotated_image.resize((1200, int(1200 * annotated_image.height / annotated_image.width))))\n",
    "\n",
    "    # Stage 2: Deep Analysis with llama-3.1-nemotron-nano-vl-8b-v1\n",
    "    # iterate through each item in the extracted data\n",
    "    for item_idx, item in enumerate(extracted_data):\n",
    "\n",
    "        # initialize the item metadata\n",
    "        item_metadata = {\n",
    "            \"extraction_id\": item_idx,\n",
    "            \"metadata\": {\"source_page\": page_num, \"type\": item.get(\"type\"), \"bbox\": item.get(\"bbox\")},\n",
    "            \"confidence\": \"N/A\",\n",
    "            \"extraction_status\": \"Success\",\n",
    "            \"data\": {}\n",
    "        }\n",
    "        item_type = item.get(\"type\")\n",
    "\n",
    "        # if the item is a picture (not a table), just save the cropped image as it is \n",
    "        if item_type == \"Picture\":\n",
    "            print(f\"[Stage 2] Found Picture (ID: {item_idx}). Triggering Specialist VLM...\")\n",
    "            bbox = item.get(\"bbox\")\n",
    "            if not bbox or not all(k in bbox for k in ['xmin', 'ymin', 'xmax', 'ymax']) or bbox[\"xmax\"] <= bbox[\"xmin\"] or bbox[\"ymax\"] <= bbox[\"ymin\"]:\n",
    "                logger.error(f\"Invalid Bounding Box for Picture (ID: {item_idx}). Skipping.\")\n",
    "                item_metadata[\"extraction_status\"] = \"Failed (Invalid Bounding Box)\"\n",
    "                page_entry[\"content\"].append(item_metadata)\n",
    "                continue\n",
    "\n",
    "            # Crop the image based on bounding box\n",
    "            width, height = page_image.size\n",
    "            crop_box = (max(0, bbox[\"xmin\"] * width), max(0, bbox[\"ymin\"] * height), min(width, bbox[\"xmax\"] * width), min(height, bbox[\"ymax\"] * height))\n",
    "\n",
    "            if crop_box[0] >= crop_box[2] or crop_box[1] >= crop_box[3]:\n",
    "                logger.error(f\"Zero-Area Bounding Box for Picture (ID: {item_idx}). Skipping.\")\n",
    "                item_metadata[\"extraction_status\"] = \"Failed (Zero-Area Bounding Box)\"\n",
    "                page_entry[\"content\"].append(item_metadata)\n",
    "                continue\n",
    "\n",
    "            cropped_img = page_image.crop(crop_box)\n",
    "            print(f\"- Cropped image: \")\n",
    "            display(cropped_img.resize((200, int(200 * cropped_img.height / cropped_img.width))))\n",
    "\n",
    "        # if the item is a table, parse the extraction results from Eclair, and reconstruct the table by converting LaTeX to HTML\n",
    "        elif item_type == \"Table\":\n",
    "            print(f\"[Stage 2] Found Table (ID: {item_idx}). Parsing and reconstructing table...\")\n",
    "\n",
    "            # Show the detected table patch for context\n",
    "            bbox = item.get(\"bbox\")\n",
    "            if bbox:\n",
    "                width, height = page_image.size\n",
    "                crop_box = (max(0, bbox[\"xmin\"] * width), max(0, bbox[\"ymin\"] * height), min(width, bbox[\"xmax\"] * width), min(height, bbox[\"ymax\"] * height))\n",
    "                if crop_box[0] < crop_box[2] and crop_box[1] < crop_box[3]:\n",
    "                    cropped_table_img = page_image.crop(crop_box)\n",
    "                    print(f\"- Cropped table patch for context:\")\n",
    "                    display(cropped_table_img.resize((600, int(600 * cropped_table_img.height / cropped_table_img.width))))\n",
    "\n",
    "            latex_code = item.get(\"text\", \"\")\n",
    "            try:\n",
    "                if not latex_code:\n",
    "                    raise ValueError(\"Empty LaTeX content for table\")\n",
    "\n",
    "                # Use the new advanced converter\n",
    "                html_table = latex_table_to_html(latex_code)\n",
    "\n",
    "                if \"<table>\" not in html_table:\n",
    "                    raise ValueError(\"tabular environment not found or conversion failed.\")\n",
    "\n",
    "                # Display the result\n",
    "                print(f\"- Reconstructed HTML Table:\")\n",
    "                display(HTML(html_table))\n",
    "\n",
    "                # Store the final HTML in the results\n",
    "                item_metadata[\"data\"] = {\"type\": \"tabular\", \"content_html\": html_table}\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"- ERROR: Failed to parse table data. Details: {e}\")\n",
    "                item_metadata[\"extraction_status\"] = \"Failed (Table Parsing Error)\"\n",
    "                item_metadata[\"data\"] = {\"error\": f\"Could not parse content: {e}\", \"raw_content\": latex_code}\n",
    "        \n",
    "        # Fallback for any other type, like 'Text'\n",
    "        else:\n",
    "            item_metadata[\"data\"] = {\"type\": \"textual\", \"content\": item.get(\"text\", \"\")}\n",
    "\n",
    "        page_entry[\"content\"].append(item_metadata)\n",
    "\n",
    "    # Save the individual JSON data for the page\n",
    "    json_filename = os.path.join(output_json_dir, f\"{filename_stem}_extracted.json\")\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(page_entry, f, indent=4)\n",
    "\n",
    "    all_document_data.append(page_entry)\n",
    "    print(f\"Annotated image saved as {output_filename}\")\n",
    "    print(f\"JSON data saved as {json_filename}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "1. Eclair sometimes will fail to segment properly (e.g. did not put a bounding box over an element)\n",
    "2. When eclair parsed tables are not always 100% accurate. It might struggle with tables that has complex layout (e.g. a lot of merged columns/rows, table within a table, etc). In such case, alternative approaches using VLM for table question-answering could be considered as opposed to trying to extract all contents of the table. \n",
    "2. Step_3 notebook talks about 2 ways to benchmark table extraction accuracy: through TEDS score (an unbiased metrics) or Q&A accuracy (biased metrics due to human-designed question-answer set. )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the output files\n",
    "\n",
    "\n",
    "Inside the `output_results` folder: \n",
    "\n",
    "- `annotate_images`: This folder contains images with bounding boxes produced by Eclair. The name of the file is `<original_image_file_name_annotated>.<image_format>`\n",
    "- `inforgraphics`:  This folder contains the results from the VLM extraction.\n",
    "- `json_output`: This folder contains results from the Eclair extracion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display an annotated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load the annotated image\n",
    "annotated_image_path = \"output_results/annotated_images/soa_1_page_1_fitz_resized_annotated.png\"\n",
    "annotated_image = Image.open(annotated_image_path)\n",
    "annotated_image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Eclair extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclair_extracted_text_path = \"output_results/json_outputs/soa_1_page_1_fitz_resized_extracted.json\"\n",
    "with open(eclair_extracted_text_path, 'r') as f:\n",
    "    eclair_extracted_text = json.load(f)\n",
    "\n",
    "print(json.dumps(eclair_extracted_text, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the extracted items one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "item = eclair_extracted_text[\"content\"][idx]\n",
    "print(\"Item ID: \", item['extraction_id'])\n",
    "print(\"Item Type: \", item['data']['type'])\n",
    "print(\"Item Content: \", item['data']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "item = eclair_extracted_text[\"content\"][idx]\n",
    "print(\"Item ID: \", item['extraction_id'])\n",
    "print(\"Item Type: \", item['data']['type'])\n",
    "print(\"Item Content: \", item['data']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "item = eclair_extracted_text[\"content\"][idx]\n",
    "print(\"Item ID: \", item['extraction_id'])\n",
    "print(\"Item Type: \", item['data']['type'])\n",
    "print(\"Item Content: \", item['data']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "item = eclair_extracted_text[\"content\"][idx]\n",
    "print(\"Item ID: \", item['extraction_id'])\n",
    "print(\"Item Type: \", item['data']['type'])\n",
    "print(\"Item Content: \", item['data']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "item = eclair_extracted_text[\"content\"][idx]\n",
    "print(\"Item ID: \", item['extraction_id'])\n",
    "print(\"Item Type: \", item['data']['type'])\n",
    "print(\"Item Content: \", item['data']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Eclair extracted tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "item = eclair_extracted_text[\"content\"][idx]\n",
    "print(\"Item ID: \", item['extraction_id'])\n",
    "print(\"Item Type: \", item['data']['type'])\n",
    "\n",
    "# when type is `tabular`, we need to display the HTML table\n",
    "display(HTML(item['data']['content_html']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting merged columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on table conversion: \n",
    "1. Some text format styles might be removed during table conversion. \n",
    "2. Visually inspecting location of merged columns/cells isn't reliable. We need to look into the HTML code.\n",
    "\n",
    "To do this, first, we export HTML code a file, so we can look at it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the html table to a file for inspecting \n",
    "os.makedirs('output_results/html_tables', exist_ok=True)\n",
    "with open('output_results/html_tables/soa_1_page_1_fitz_resized_extracted.html', 'w') as f:\n",
    "    f.write(item['data']['content_html'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For merged columns, look for the `<td colspan=\"2\">` tag. \n",
    "\n",
    "\n",
    "**Example 1**: \n",
    "\n",
    "look at the `soa_1_page_1_fitz_extracted.html` file we exported above. Find this line: \n",
    "```html\n",
    "<tr>\n",
    "    <td colspan=\"2\"><b>Pre-Treatment Phase</b></td>\n",
    "    <td colspan=\"8\"><b>Treatment Phase1</b></td>\n",
    "    <td colspan=\"3\"><b>Post-Treatment Phase</b></td>\n",
    "</tr>\n",
    "```\n",
    "\n",
    "The `colspan` is the number of column units that this header spans across. For example: \n",
    "\n",
    "1. `Pre-Treatment Phase` spans 2 columns, meaning that it covers these columns: \n",
    "    -  `Day within Cycle Visit Window (± days)`\n",
    "    - `D-14 to D-1` \n",
    "2. `Treatment Phase1` spans 8 columns, meaning that it covers these columns: \n",
    "    - `D1`\n",
    "    - `D8 (±2)`\n",
    "    - `D15 (±2)`\n",
    "    - `D22 (±2)`\n",
    "    - `D1`\n",
    "    - `D15 (±2)`\n",
    "    - `D1`\n",
    "    - `D15 (±2)`\n",
    "3. `Post-Treatment Phase` spans 3 columns, meaning that it covers these columns: \n",
    "    - `≤ 10 d from the decision of trial treatment withdrawal`\n",
    "    - `1 month after last dose of trial treatment (28+7d)`\n",
    "    - a column with empty header at the end of the table (this was incorrectly generated)\n",
    "\n",
    "\n",
    "**Example 2**: \n",
    "\n",
    "Find this block in the same file above: \n",
    "\n",
    "```html\n",
    "<tr>\n",
    "    <td><b>Cycle Number</b></td>\n",
    "    <td><b>Screening</b></td>\n",
    "    <td colspan=\"4\"><b>Cycle 1</b></td>\n",
    "    <td colspan=\"2\"><b>Cycle 2, 4, 6 etc.</b></td>\n",
    "    <td colspan=\"2\"><b>Cycle 3, 5, 7 etc.</b></td>\n",
    "    <td><b>EOT</b></td>\n",
    "    <td><b>1M FUP</b></td>\n",
    "</tr>\n",
    "```\n",
    "\n",
    "We can see that: \n",
    "- `Cycle Number` spans 1 column. It covers: \n",
    "    - `Day within Cycle Visit Window (± days)`\n",
    "- `Screening` spans 1 column. It covers: \n",
    "    - `D-14 to D-1` \n",
    "- `Cycle 1` spans 4 columns, It covers: \n",
    "    - `D1`\n",
    "    - `D8 (±2)`\n",
    "    - `D15 (±2)`\n",
    "    - `D22 (±2)`\n",
    "- `Cycle 2, 4, 6 etc.` spans 2 colums. It covers: \n",
    "    - `D1`\n",
    "    - `D15 (±2)`\n",
    "- `Cycle 3, 5, 7 etc.` spans 2 columns. It covers: \n",
    "    -  `D1`\n",
    "    - `D15 (±2)`\n",
    "- `EOT` spans 1 column. It covers: \n",
    "    - `≤ 10 d from the decision of trial treatment withdrawal`\n",
    "- `1M FUP` spans 1 column. It covers: \n",
    "    - `1 month after last dose of trial treatment (28+7d)`\n",
    "\n",
    "So even though the HTML displays the headers a bit skrewed (possibly due to alignment settings), the actual column grouping is correct (except for the last extra column without header)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect merged rows\n",
    "\n",
    "Similarly to colspan, merged rows are displayed with `rowspan` tag, like this: \n",
    "```html\n",
    "<th rowspan=\"2\">SOME WORDS HERE</th>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```html\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
